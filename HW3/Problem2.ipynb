{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Use MPS on macOS if available, else GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hugging Face \"imdb\" dataset loads a dictionary with \"train\" and \"test\"\n",
    "imdb = load_dataset(\"imdb\")\n",
    "# imdb['train'] has 25k examples; imdb['test'] has 25k examples\n",
    "print(imdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 22500\n",
      "Val   set size: 2500\n",
      "Test  set size: 25000\n"
     ]
    }
   ],
   "source": [
    "train_texts = imdb['train']['text']\n",
    "train_labels = imdb['train']['label']\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify= train_labels\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_texts)}\")\n",
    "print(f\"Val   set size: {len(val_texts)}\")\n",
    "print(f\"Test  set size: {len(imdb['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll use the standard \"bert-base-uncased\" tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Because IMDB reviews can be somewhat long, let's allow up to e.g. 256 tokens\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "# Tokenize each split\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings   = tokenize_function(val_texts)\n",
    "test_encodings  = tokenize_function(imdb['test']['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized outputs into PyTorch Datasets\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset   = IMDbDataset(val_encodings,   val_labels)\n",
    "test_dataset  = IMDbDataset(test_encodings,  imdb['test']['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT encoder (frozen) + 3-layer feedforward classifier on top.\n",
    "    Output dimension = 2 (positive vs. negative).\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name=\"bert-base-uncased\", num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained BERT\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # Freeze all BERT weights so we only train the classifier layers\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        # 4-layer classifier: 768 -> 512 -> 256 -> 128 -> 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)  # 2 classes for IMDB (pos vs neg)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get last-layer hidden states & pooler from BERT\n",
    "        # outputs: BaseModelOutputWithPoolingAndNoAttention\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # \"pooled_output\" is the [CLS] representation\n",
    "        pooled_output = outputs.pooler_output  # shape [batch_size, 768]\n",
    "        \n",
    "        # Pass pooled_output through our classifier\n",
    "        logits = self.classifier(pooled_output)  # shape [batch_size, 2]\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBERTModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate our custom model\n",
    "model = CustomBERTModel(pretrained_model_name=\"bert-base-uncased\", num_classes=2)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Dataloaders ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters = 558210\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# We'll use cross-entropy for 2-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only the classifier params are trainable. Let's confirm:\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"Number of trainable parameters = {sum(p.numel() for p in trainable_params)}\")\n",
    "\n",
    "optimizer = optim.Adam(trainable_params, lr=1e-4)\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]\n",
      "  Train Loss: 0.5607 | Train Acc: 0.7029\n",
      "  Val   Loss: 0.4592 | Val   Acc: 0.7892\n",
      "-------------------------------------------------------\n",
      "Epoch [2/2]\n",
      "  Train Loss: 0.4800 | Train Acc: 0.7719\n",
      "  Val   Loss: 0.4267 | Val   Acc: 0.8020\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "EPOCHS = 2  # For demo; you might want more in practice\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids      = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels         = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids, attention_mask)  # [batch_size, 2]\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        total_train_correct += correct\n",
    "        total_examples += labels.size(0)\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_acc = total_train_correct / total_examples\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Validation\n",
    "    # ------------------------------\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    total_val_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids      = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels         = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_val_correct += correct\n",
    "            total_val_examples += labels.size(0)\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_acc = total_val_correct / total_val_examples\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val   Loss: {avg_val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "    print(\"-------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.8050  (on 25000 test samples)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "total_test_correct = 0\n",
    "total_test_examples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids      = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels         = batch['labels'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        \n",
    "        total_test_correct += correct\n",
    "        total_test_examples += labels.size(0)\n",
    "\n",
    "test_acc = total_test_correct / total_test_examples\n",
    "print(f\"Test Accuracy = {test_acc:.4f}  (on {total_test_examples} test samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (0=NEG, 1=POS): [1 0]\n",
      "\n",
      "Review: This movie was absolutely wonderful! The acting was incredible and I loved it.\n",
      " => Sentiment: POSITIVE\n",
      "\n",
      "Review: Terrible film. The script was awful and the characters were boring.\n",
      " => Sentiment: NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# let's try it\n",
    "example_texts = [\n",
    "    \"This movie was absolutely wonderful! The acting was incredible and I loved it.\",\n",
    "    \"Terrible film. The script was awful and the characters were boring.\"\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "tokens = tokenizer(\n",
    "    example_texts, \n",
    "    max_length=256, \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(tokens[\"input_ids\"], tokens[\"attention_mask\"])\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "print(\"Predictions (0=NEG, 1=POS):\", predictions)\n",
    "for text, pred in zip(example_texts, predictions):\n",
    "    sentiment = \"POSITIVE\" if pred == 1 else \"NEGATIVE\"\n",
    "    print(f\"\\nReview: {text}\\n => Sentiment: {sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
