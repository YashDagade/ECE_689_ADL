{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#############################\n",
    "# 1) Hyperparameters & Data\n",
    "#############################\n",
    "BATCH_SIZE       = 64\n",
    "EPOCHS           = 15          # limited to 15\n",
    "LR               = 2e-4        # keep a stable LR\n",
    "NR_RESNET        = 5           # enough for 7×7\n",
    "NR_FILTERS       = 80          # enough filters\n",
    "NR_LOGISTIC_MIX  = 5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() \n",
    "                      else 'mps' if torch.backends.mps.is_available()\n",
    "                      else 'cpu')\n",
    "PRINT_EVERY      = 100\n",
    "IMG_SIZE         = 7  # 7×7 downsampled MNIST\n",
    "\n",
    "def lighter_dequantize(x):\n",
    "    \"\"\"\n",
    "    A milder random dequant: \n",
    "      scale from [0,1] to [0..255], then add noise in [-0.5, 0.5], \n",
    "      then clamp to [0..255], then /256 => [0..1].\n",
    "    \"\"\"\n",
    "    # x in [0,1], shape (1,H,W)\n",
    "    x255 = x * 255.\n",
    "    noise = torch.rand_like(x255) - 0.5  # in [-0.5,0.5]\n",
    "    x255_noisy = (x255 + noise).clamp_(0., 255.)\n",
    "    return x255_noisy / 256.\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((7,7)),\n",
    "    transforms.ToTensor(),          \n",
    "    transforms.Lambda(lighter_dequantize),\n",
    "    transforms.Lambda(lambda x: x*2.0 - 1.0)  # map [0,1] -> [-1,1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='mnist_data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(\n",
    "    root='mnist_data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "#############################\n",
    "# 2) Shifted Convs\n",
    "#############################\n",
    "class DownShiftedConv2d(nn.Module):\n",
    "    def __init__(self, inC, outC, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(inC, outC, kernel_size, stride=1, padding=1)\n",
    "    def forward(self, x):\n",
    "        # pad top, then remove bottom row\n",
    "        out = F.pad(x, (0,0,1,0))\n",
    "        out = self.conv(out)\n",
    "        return out[:,:,:-1,:]\n",
    "\n",
    "class DownRightShiftedConv2d(nn.Module):\n",
    "    def __init__(self, inC, outC, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(inC, outC, kernel_size, stride=1, padding=1)\n",
    "    def forward(self, x):\n",
    "        # pad left+top, then remove bottom row & right column\n",
    "        out = F.pad(x, (1,0,1,0))\n",
    "        out = self.conv(out)\n",
    "        return out[:,:,:-1,:-1]\n",
    "\n",
    "#############################\n",
    "# 3) Network-in-Network & GatedResnet\n",
    "#############################\n",
    "class Nin(nn.Module):\n",
    "    def __init__(self, inC, outC):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(inC, outC)\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        x = x.permute(0,2,3,1).contiguous()  # => (B,H,W,C)\n",
    "        x = x.view(B*H*W, C)\n",
    "        x = self.lin(x)\n",
    "        x = x.view(B,H,W,-1)\n",
    "        x = x.permute(0,3,1,2).contiguous()\n",
    "        return x\n",
    "\n",
    "def concat_elu(t):\n",
    "    return F.elu(torch.cat([t, -t], dim=1))\n",
    "\n",
    "class GatedResnet(nn.Module):\n",
    "    \"\"\"\n",
    "    PixelCNN++-style gated residual block, skip_conn=1 => use extra nin skip.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filters, conv_op, skip_conn=1):\n",
    "        super().__init__()\n",
    "        self.skip_conn = skip_conn\n",
    "        self.conv_in = conv_op(2*n_filters, n_filters)\n",
    "        if skip_conn:\n",
    "            self.nin_skip = Nin(n_filters, n_filters)\n",
    "        self.conv_out = conv_op(2*n_filters, 2*n_filters)\n",
    "\n",
    "    def forward(self, x, a=None):\n",
    "        c = self.conv_in(concat_elu(x))\n",
    "        if self.skip_conn and a is not None:\n",
    "            c = c + self.nin_skip(a)\n",
    "        c = concat_elu(c)\n",
    "        c = self.conv_out(c)\n",
    "        a_, b_ = torch.chunk(c, 2, dim=1)\n",
    "        return x + a_ * torch.sigmoid(b_)\n",
    "\n",
    "#############################\n",
    "# 4) Single-Level PixelCNN++\n",
    "#############################\n",
    "class PixelCNNpp(nn.Module):\n",
    "    \"\"\"\n",
    "    Single-level PixelCNN++ for 7x7.\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_resnet=5, nr_filters=80, nr_logistic_mix=5, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.nr_filters = nr_filters\n",
    "        self.nr_logistic_mix = nr_logistic_mix\n",
    "\n",
    "        # initial \"down\" and \"down-right\" conv\n",
    "        self.u_init  = DownShiftedConv2d(in_channels+1, nr_filters)\n",
    "        self.ul_init = DownRightShiftedConv2d(in_channels+1, nr_filters)\n",
    "\n",
    "        self.resnet_v = nn.ModuleList([\n",
    "            GatedResnet(nr_filters, DownShiftedConv2d, skip_conn=0) \n",
    "            for _ in range(nr_resnet)\n",
    "        ])\n",
    "        self.resnet_h = nn.ModuleList([\n",
    "            GatedResnet(nr_filters, DownRightShiftedConv2d, skip_conn=1) \n",
    "            for _ in range(nr_resnet)\n",
    "        ])\n",
    "        self.nin_out = Nin(nr_filters, 3*nr_logistic_mix)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.size()\n",
    "        # channel of ones\n",
    "        ones = torch.ones(B,1,H,W, device=x.device)\n",
    "        x_aug = torch.cat([x, ones], dim=1)\n",
    "\n",
    "        v = self.u_init(x_aug)\n",
    "        h = self.ul_init(x_aug)\n",
    "        # pass through the stack of gated resnets\n",
    "        for rv, rh in zip(self.resnet_v, self.resnet_h):\n",
    "            v = rv(v)\n",
    "            h = rh(h, a=v)\n",
    "\n",
    "        out = self.nin_out(F.elu(h))\n",
    "        return out\n",
    "\n",
    "#############################\n",
    "# 5) Mixture of Logistics Loss\n",
    "#############################\n",
    "def log_sum_exp(x, axis=-1):\n",
    "    m,_ = x.max(dim=axis, keepdim=True)\n",
    "    return m + (x - m).exp().sum(dim=axis, keepdim=True).log()\n",
    "\n",
    "def log_prob_from_logits(x):\n",
    "    m,_ = x.max(dim=-1, keepdim=True)\n",
    "    x0  = x - m\n",
    "    return x0 - x0.exp().sum(dim=-1, keepdim=True).log()\n",
    "\n",
    "def discretized_mix_logistic_loss_1d(x, l):\n",
    "    \"\"\"\n",
    "    x: (B,1,H,W), l: (B,3*nr_mix,H,W).\n",
    "    We'll reorder to (B,H,W,C).\n",
    "    \"\"\"\n",
    "    x = x.permute(0,2,3,1)  # => (B,H,W,1)\n",
    "    l = l.permute(0,2,3,1)  # => (B,H,W,3*nr_mix)\n",
    "    nr_mix = l.shape[-1] // 3\n",
    "\n",
    "    logit_probs = l[...,:nr_mix]\n",
    "    means       = l[...,nr_mix:2*nr_mix]\n",
    "    log_scales  = l[...,2*nr_mix:3*nr_mix].clamp(min=-7.)\n",
    "\n",
    "    log_probs = log_prob_from_logits(logit_probs)  # shape (B,H,W,nr_mix)\n",
    "\n",
    "    x = x.unsqueeze(-1)                # => (B,H,W,1,1)\n",
    "    means = means.unsqueeze(3)         # => (B,H,W,nr_mix,1)\n",
    "    log_scales = log_scales.unsqueeze(3)\n",
    "    scales = log_scales.exp()\n",
    "\n",
    "    centered_x = x - means\n",
    "    inv_stdv   = 1./scales\n",
    "    plus_in    = inv_stdv*(centered_x + 1./255.)\n",
    "    cdf_plus   = torch.sigmoid(plus_in)\n",
    "    min_in     = inv_stdv*(centered_x - 1./255.)\n",
    "    cdf_min    = torch.sigmoid(min_in)\n",
    "    cdf_delta  = cdf_plus - cdf_min\n",
    "\n",
    "    log_cdf_plus  = plus_in - F.softplus(plus_in)\n",
    "    log_one_minus_cdf_min = -F.softplus(min_in)\n",
    "    mid_in        = inv_stdv*centered_x\n",
    "    log_pdf_mid   = mid_in - log_scales - 2.*F.softplus(mid_in)\n",
    "\n",
    "    mask_left   = (x < -0.999).float()\n",
    "    mask_right  = (x >  0.999).float()\n",
    "    mask_center = 1. - (mask_left + mask_right)\n",
    "\n",
    "    left_out  = log_cdf_plus[...,0]\n",
    "    right_out = log_one_minus_cdf_min[...,0]\n",
    "    center_out= torch.log(torch.clamp(cdf_delta[...,0], min=1e-12))\n",
    "    cond_cdelta = (cdf_delta[...,0] > 1e-5).float()\n",
    "    center_out  = cond_cdelta*center_out + (1.-cond_cdelta)*(log_pdf_mid[...,0] - math.log(127.5))\n",
    "\n",
    "    out = mask_left*left_out + mask_right*right_out + mask_center*center_out\n",
    "    out = out + log_probs[...,0,:]  # add mixing weights\n",
    "    # log-sum-exp across mixtures\n",
    "    out = torch.logsumexp(out, dim=-1)  # => (B,H,W)\n",
    "    return -out.mean()\n",
    "\n",
    "#############################\n",
    "# 6) Train/Eval\n",
    "#############################\n",
    "def train_one_epoch(model, loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for i,(imgs,_) in enumerate(tqdm(loader, desc=f\"Epoch {epoch}\")):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out  = model(imgs)\n",
    "        loss = discretized_mix_logistic_loss_1d(imgs, out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        if (i+1) % PRINT_EVERY == 0:\n",
    "            print(f\"   step {i+1}/{len(loader)} - batch loss={loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    for imgs,_ in loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        out  = model(imgs)\n",
    "        loss = discretized_mix_logistic_loss_1d(imgs, out)\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "#############################\n",
    "# 7) Sampling with Temperature\n",
    "#############################\n",
    "@torch.no_grad()\n",
    "def sample_from_discretized_mix_logistic_1d(l, nr_mix, temperature=0.85):\n",
    "    \"\"\"\n",
    "    We incorporate a 'temperature' factor to reduce randomness, \n",
    "    hopefully producing more structured samples.\n",
    "    \"\"\"\n",
    "    B, C = l.shape\n",
    "    logit_probs = l[:, :nr_mix] / temperature\n",
    "    means       = l[:, nr_mix:2*nr_mix]\n",
    "    log_scales  = (l[:, 2*nr_mix:3*nr_mix] / temperature).clamp(min=-7.)\n",
    "\n",
    "    probs       = F.softmax(logit_probs, dim=1)\n",
    "    # pick mixture component\n",
    "    cumprobs    = torch.cumsum(probs, dim=1)\n",
    "    rand_cat    = torch.rand(B, device=l.device)\n",
    "    cat_idx     = torch.zeros(B, dtype=torch.long, device=l.device)\n",
    "    for i in range(nr_mix):\n",
    "        cat_idx = torch.where(rand_cat<=cumprobs[:,i],\n",
    "                              torch.full_like(cat_idx, i), \n",
    "                              cat_idx)\n",
    "    sel_means   = means.gather(1, cat_idx.unsqueeze(1)).squeeze(1)\n",
    "    sel_scales  = log_scales.gather(1, cat_idx.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # sample logistic noise\n",
    "    u = torch.rand(B, device=l.device)\n",
    "    x = sel_means + torch.exp(sel_scales)*(torch.log(u) - torch.log(1.-u))\n",
    "    x = x.clamp(-1.,1.)\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_model(model, batch_size=16, image_size=7, nr_mix=5, device='cpu'):\n",
    "    \"\"\"Autoregressive sampling: fill row-by-row, col-by-col in [-1,1].\"\"\"\n",
    "    model.eval()\n",
    "    x = torch.zeros(batch_size,1,image_size,image_size,device=device)\n",
    "    for row in range(image_size):\n",
    "        for col in range(image_size):\n",
    "            out = model(x)\n",
    "            l   = out[:,:,row,col]  # => (B,3*nr_mix)\n",
    "            px  = sample_from_discretized_mix_logistic_1d(l, nr_mix, temperature=0.85)\n",
    "            x[:,0,row,col] = px\n",
    "    return x\n",
    "\n",
    "def sample_and_show(model, batch_size=16, nr_mix=5):\n",
    "    samples = sample_model(model, batch_size=batch_size, image_size=7, nr_mix=nr_mix, device=DEVICE)\n",
    "    # map [-1,1]->[0,1]\n",
    "    samples = (samples + 1) / 2\n",
    "    samples = samples.clamp(0,1)\n",
    "\n",
    "    import math\n",
    "    nrow = int(math.sqrt(batch_size))\n",
    "    ncol = int(math.ceil(batch_size / nrow))\n",
    "    fig,axes = plt.subplots(nrow,ncol,figsize=(ncol*2,nrow*2))\n",
    "    axes = axes.flatten() if batch_size>1 else [axes]\n",
    "    for i in range(batch_size):\n",
    "        if i >= len(axes): \n",
    "            break\n",
    "        img = samples[i,0].cpu().numpy()\n",
    "        axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#############################\n",
    "# 8) Main\n",
    "#############################\n",
    "def main():\n",
    "    model = PixelCNNpp(\n",
    "        nr_resnet=NR_RESNET, \n",
    "        nr_filters=NR_FILTERS, \n",
    "        nr_logistic_mix=NR_LOGISTIC_MIX,\n",
    "        in_channels=1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    print(f\"Training PixelCNN++ on 7×7 MNIST for {EPOCHS} epochs. Device={DEVICE}\")\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_nll = train_one_epoch(model, train_loader, optimizer, epoch)\n",
    "        test_nll  = evaluate(model, test_loader)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} => train NLL={train_nll:.4f}, test NLL={test_nll:.4f}\")\n",
    "\n",
    "    print(\"Done training! Generating samples...\")\n",
    "    sample_and_show(model, batch_size=16, nr_mix=NR_LOGISTIC_MIX)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/937 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 248\u001b[0m\n\u001b[1;32m    245\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits_for_loss, targets_for_loss)\n\u001b[1;32m    247\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 248\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    251\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# 1) Imports and device #\n",
    "#########################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "##############################\n",
    "# 2) Define WaveNet classes #\n",
    "##############################\n",
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    \"\"\"\n",
    "    1D causal convolution by left-padding, then cutting off\n",
    "    to ensure position t sees no future positions t+1, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=2, dilation=1):\n",
    "        self._padding = (kernel_size - 1) * dilation\n",
    "        super().__init__(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=self._padding\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        out = super().forward(x)\n",
    "        # slice off right padding\n",
    "        if self._padding > 0:\n",
    "            out = out[:, :, :-self._padding]\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One WaveNet residual block with gating:\n",
    "      filter: tanh\n",
    "      gate: sigmoid\n",
    "      skip + residual outputs\n",
    "    \"\"\"\n",
    "    def __init__(self, residual_channels, dilation_channels, skip_channels,\n",
    "                 kernel_size=2, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv_filter = CausalConv1d(\n",
    "            residual_channels, dilation_channels,\n",
    "            kernel_size=kernel_size, dilation=dilation\n",
    "        )\n",
    "        self.conv_gate = CausalConv1d(\n",
    "            residual_channels, dilation_channels,\n",
    "            kernel_size=kernel_size, dilation=dilation\n",
    "        )\n",
    "        self.conv_res  = nn.Conv1d(dilation_channels, residual_channels, 1)\n",
    "        self.conv_skip = nn.Conv1d(dilation_channels, skip_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, residual_channels, T)\n",
    "        f = torch.tanh(self.conv_filter(x))\n",
    "        g = torch.sigmoid(self.conv_gate(x))\n",
    "        out = f * g  # (B, dilation_channels, T)\n",
    "\n",
    "        skip = self.conv_skip(out)  # (B, skip_channels, T)\n",
    "        res  = self.conv_res(out)   # (B, residual_channels, T)\n",
    "        return skip, x + res\n",
    "\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    \"\"\"\n",
    "    WaveNet-like model for discrete 1D sequences of length 49 (7x7).\n",
    "    Produces logits over 256 classes at each position.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels=256,\n",
    "                 residual_channels=32,\n",
    "                 dilation_channels=32,\n",
    "                 skip_channels=32,\n",
    "                 dilations=[1,2,4,8],\n",
    "                 kernel_size=2,\n",
    "                 out_channels=256):\n",
    "        super().__init__()\n",
    "        # 1) Causal front-end (from one-hot depth -> residual_channels)\n",
    "        self.causal = CausalConv1d(\n",
    "            in_channels, residual_channels,\n",
    "            kernel_size=kernel_size, dilation=1\n",
    "        )\n",
    "\n",
    "        # 2) Dilated residual blocks\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        for d in dilations:\n",
    "            self.res_blocks.append(\n",
    "                ResidualBlock(\n",
    "                    residual_channels,\n",
    "                    dilation_channels,\n",
    "                    skip_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=d\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 3) Post-processing to final 256 logits\n",
    "        self.postprocess1 = nn.Conv1d(skip_channels, skip_channels, 1)\n",
    "        self.postprocess2 = nn.Conv1d(skip_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T), pixel values in [0..255]\n",
    "        Return: logits shape (B, T, 256)\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "\n",
    "        # One-hot => (B,T,256) => permute => (B,256,T)\n",
    "        x_ohe = F.one_hot(x, num_classes=256).float()\n",
    "        x_ohe = x_ohe.permute(0,2,1)\n",
    "\n",
    "        # Causal conv => (B, residual_channels, T)\n",
    "        out = self.causal(x_ohe)\n",
    "\n",
    "        # Accumulate skip outputs\n",
    "        skip_sum = 0\n",
    "        for block in self.res_blocks:\n",
    "            skip, out = block(out)\n",
    "            skip_sum = skip_sum + skip  # broadcast sum\n",
    "\n",
    "        # Post-process => (B,256,T)\n",
    "        out = F.relu(skip_sum)\n",
    "        out = self.postprocess1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.postprocess2(out)\n",
    "\n",
    "        # => (B,T,256)\n",
    "        out = out.permute(0,2,1).contiguous()\n",
    "        return out\n",
    "\n",
    "    def generate(self, seq_len=49):\n",
    "        \"\"\"\n",
    "        Autoregressive generation. We'll start with a single 0 pixel (seed),\n",
    "        feed it, sample next pixel, append, repeat. Return (seq_len,).\n",
    "        \"\"\"\n",
    "        current_seq = torch.zeros((1,1), dtype=torch.long, device=device)  # shape (1,1)\n",
    "        for _ in range(seq_len):\n",
    "            # logits => (1,current_length,256)\n",
    "            logits = self.forward(current_seq)\n",
    "            last_logits = logits[:, -1, :]  # (1,256)\n",
    "            probs = F.softmax(last_logits, dim=-1)\n",
    "            next_pixel = torch.multinomial(probs, 1)  # (1,1)\n",
    "            current_seq = torch.cat([current_seq, next_pixel], dim=1)\n",
    "\n",
    "        # skip the seed => shape (seq_len,)\n",
    "        return current_seq[0, 1:]\n",
    "\n",
    "\n",
    "#############################\n",
    "# 3) Load and prepare MNIST #\n",
    "#############################\n",
    "\n",
    "# We'll do standard MNIST, resized to 7x7 => flatten => 49 pixels\n",
    "transform_7x7 = transforms.Compose([\n",
    "    transforms.Resize((7, 7)),\n",
    "    transforms.ToTensor()  # => (1,7,7) in [0,1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='.', train=True, download=True, transform=transform_7x7)\n",
    "test_dataset  = datasets.MNIST(root='.', train=False, download=True, transform=transform_7x7)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "####################################\n",
    "# 4) Instantiate and train WaveNet #\n",
    "####################################\n",
    "\n",
    "model = WaveNet(\n",
    "    in_channels=256,\n",
    "    residual_channels=32,\n",
    "    dilation_channels=32,\n",
    "    skip_channels=32,\n",
    "    dilations=[1,2,4,8],\n",
    "    kernel_size=2,\n",
    "    out_channels=256\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for images, _ in pbar:\n",
    "        # images => shape (B,1,7,7)\n",
    "        B_ = images.size(0)\n",
    "\n",
    "        # Flatten => (B,49), then move to device\n",
    "        images = images.view(B_, -1).to(device)  # => (B,49)\n",
    "\n",
    "        # Scale to [0..255], cast to long\n",
    "        images = (images * 255).long()  # => (B,49)\n",
    "\n",
    "        # Forward => (B,49,256)\n",
    "        logits = model(images)\n",
    "\n",
    "        # ---- SHIFT-BY-1: \"predict the next pixel\"  ----\n",
    "        # We do it in a simpler flattened approach:\n",
    "        # Flatten all time => shape (B*49, 256)\n",
    "        # logits_flat = logits.contiguous().view(-1, 256)  # => (B*49,256)\n",
    "\n",
    "        # # Flatten images => (B,49) => (B*49)\n",
    "        # targets_flat = images.contiguous().view(-1)      # => (B*49,)\n",
    "\n",
    "        # # For the 'next pixel' approach, skip the last logit, skip the first target\n",
    "        # # so logits_for_loss[i] tries to predict targets_for_loss[i].\n",
    "        # logits_for_loss = logits_flat[:-1]  # => (B*49 - 1, 256)\n",
    "        # targets_for_loss = targets_flat[1:] # => (B*49 - 1,)\n",
    "        \n",
    "        \n",
    "        #------\n",
    "        B = images.size(0)\n",
    "        # Make tensors contiguous before reshaping\n",
    "        logits = logits.contiguous()\n",
    "        logits_flat = logits.view(B * 49, 256)  # => (B*49,256)\n",
    "        targets_flat = images.view(-1)          # => (B*49,)\n",
    "        \n",
    "        # Remove batch_size-1 elements from both tensors\n",
    "        logits_for_loss = logits_flat[:-B]        # => (B*48, 256)\n",
    "        targets_for_loss = targets_flat[B:]        # => (B*48,)\n",
    "        \n",
    "        #------\n",
    "\n",
    "        loss = F.cross_entropy(logits_for_loss, targets_for_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - avg_loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "############################################\n",
    "# 5) Sample and visualize synthetic images #\n",
    "############################################\n",
    "\n",
    "model.eval()\n",
    "num_samples = 8\n",
    "generated_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_samples):\n",
    "        seq = model.generate(seq_len=49)  # shape (49,)\n",
    "        generated_list.append(seq.cpu().numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, num_samples, figsize=(num_samples*2,2))\n",
    "for i, gen_seq in enumerate(generated_list):\n",
    "    # Reshape => 7x7\n",
    "    img = gen_seq.reshape(7,7)\n",
    "    axes[i].imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
