{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training PixelCNN on MNIST...\n",
      "Epoch 1/5, NLL = 0.1142\n",
      "Epoch 2/5, NLL = 0.0833\n",
      "Epoch 3/5, NLL = 0.0819\n",
      "Epoch 4/5, NLL = 0.0813\n",
      "Epoch 5/5, NLL = 0.0809\n",
      "Test NLL = 62.9772\n",
      "Sampling from PixelCNN...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAH2CAYAAABHmTQtAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFvZJREFUeJzt3X2Q1WX5+PHrCLuAtjKBgAQpoo1togmpSKRYqAQyMuMkGDY85FRWZjo6+ARSgDqMZOk4pgmKNioDaFhrpE5CQwWJk0mCNmNjpebDRD4EKAN4f//g5/5YWWARlmt3eb1mmIGzn3PO55yFfXPde5+zlVJKCQBgnzsg+wQAYH8lwgCQRIQBIIkIA0ASEQaAJCIMAElEGACSiDAAJBFhAEgiwrRIc+fOjUqlUv+rffv20bt375g4cWK88sor9cdNmDAh+vTp02znsXTp0qhUKrF06dLtPrZs2bIYPXp09OrVK6qrq6Nz587x+c9/Pn7605/G+vXr64/r06dPVCqVuPDCC3d4+wsXLqy/7IPH3rFjx/jnP/+53XVOO+206Nev3y7PvZQS8+bNi1NOOSW6d+8eHTt2jN69e8ewYcNi9uzZTXwG8vTp0ycmTJiQfRrQrESYFu3uu++O5cuXx+OPPx7f+MY34oEHHohTTjmlPnJTpkyJX/ziF/v8vKZOnRqnnnpqvPLKKzF9+vR4/PHHY968eTF06ND4wQ9+EJMnT97uOnPmzIm//e1vTb6PjRs3Nno7TXXVVVfFV7/61aitrY3Zs2fH4sWLY8aMGdGjR494+OGHP/LtAntP++wTgJ3p169fnHDCCRER8cUvfjG2bNkS06dPj0WLFsX5558fRx555D4/pwULFsS0adPiggsuiDvvvDMqlUr9x4YPHx6TJk2K5cuXN7jOoEGDYs2aNXH11VfHgw8+2KT7+fKXvxz3339/XH755fHZz352t87x3XffjZ/85Ccxbty4+NnPftbgYxMmTIj3339/t24PaB4mYVqVk08+OSKifpn2w8vR8+bNi0qlErfeemuD602dOjXatWsXjz/+eP1lTz31VJx99tnRpUuX6NixY/Tv3z/mz5+/y3OYNm1afPzjH49bbrmlQYA/UFNTE2eeeWaDy7p06RJXXnllPPTQQ7FixYomPdZJkyZF165d44orrmjS8dtav359bNy4MXr27Nnoxw84oOE//R/+8IcxcODA6NKlSxx88MExYMCAmDNnTnz457v06dMnRo4cGXV1ddG/f//o1KlT1NbWRl1dXURsXUqvra2Ngw46KE466aR46qmnGlx/woQJ8bGPfSxWr14dQ4cOjYMOOii6desWF110UWzYsGGXj+udd96Jyy+/PI444oiorq6OXr16xSWXXNJg+T9i63+UBg4cGJ07d44DDzww+vbtG1//+td3efuwr4kwrcoLL7wQERHdunVr9OPnnXdeXHjhhXHZZZfVB+CJJ56IGTNmxNVXXx1nnHFGREQsWbIkBg8eHG+99Vbcfvvt8fDDD8fxxx8fY8aMiblz5+7w/l999dV49tln48wzz4wDDzxwt879+9//fvTq1SsmTZrUpONrampi8uTJ8eijj8YTTzyxW/d1yCGHxFFHHRW33XZb3HTTTfH8889vF9Rt/eMf/4hvfetbMX/+/HjooYfinHPOie9973sxffr07Y595pln4qqrroorrrgiHnrooejcuXOcc845MXXq1Jg9e3Zcf/31cd9998Xbb78dI0eOjHfffbfB9Tdt2hQjRoyIoUOHxqJFi+Kiiy6KO+64I8aMGbPTx7Rhw4YYMmRI3HPPPXHxxRfH4sWL44orroi5c+fG2WefXf/4li9fHmPGjIm+ffvGvHnz4pFHHolrr702Nm/evFvPIewTBVqgu+++u0REWbFiRdm0aVP53//+V+rq6kq3bt1KTU1Nee2110oppYwfP74cfvjhDa773nvvlf79+5cjjjiirFmzpvTo0aMMGTKkbN68uf6YT3/606V///5l06ZNDa47cuTI0rNnz7Jly5ZSSilLliwpEVGWLFlSSillxYoVJSLKlVde2eTHcvjhh5ezzjqrlFLKnXfeWSKi/OpXv2pw+wsWLNjusa9cubJs3Lix9O3bt5xwwgnl/fffL6WUMmTIkHLMMcfs8n6ffPLJcthhh5WIKBFRampqysiRI8u9995bf1uN2bJlS9m0aVOZNm1a6dq1a4NjDz/88NKpU6fy8ssv11/2l7/8pURE6dmzZ1m/fn395YsWLSoRUX75y1/WXzZ+/PgSEeXmm29ucJ/XXXddiYjy+9//vsF9jR8/vv7PN9xwQznggAPKypUrG1x34cKFJSLKr3/961JKKbNmzSoRUd56661dPkeQzSRMi3byySdHVVVV1NTUxMiRI+PQQw+NxYsXR48ePXZ4nQ4dOsT8+fNj7dq1MWDAgCilxAMPPBDt2rWLiK3T9PPPPx/nn39+RERs3ry5/teIESPi1Vdf3a0NVLtj4sSJ8ZnPfCauvPLKJn1ftrq6OmbMmBFPPfVUk5bKt3XiiSfGCy+8EL/5zW/i6quvjkGDBsVvf/vbGDduXIPJMWLrasHpp58enTt3jnbt2kVVVVVce+21sXbt2njjjTca3O7xxx8fvXr1qv9zbW1tRGzdtb3t6sAHlze2w/uD5/4DY8eOjYitKxQ7UldXF/369Yvjjz++weds2LBhDXawn3jiiRERMXr06Jg/f36D3fTQ0ogwLdq9994bK1eujKeffjr+/e9/x6pVq2Lw4MG7vN5RRx0Vp5xySrz33ntx/vnnN/je6Ouvvx4REZdffnlUVVU1+PWd73wnIiL+85//NHq7hx12WEREvPjiix/p8bRr1y6uv/76WL16ddxzzz1Nus55550XAwYMiGuuuSY2bdq0W/dXVVUVw4YNi+uuuy4effTReOmll+K0006Lurq6WLx4cUREPPnkk/Xfw77zzjvjD3/4Q6xcuTKuueaaiIjtlpO7dOnS4M/V1dU7vfy9995rcHn79u2ja9euDS479NBDIyJi7dq1O3wsr7/+eqxatWq7z1lNTU2UUuo/Z6eeemosWrQoNm/eHOPGjYvevXtHv3794oEHHtjFswX7nt3RtGi1tbX1u6N3x+zZs+ORRx6Jk046KW699dYYM2ZMDBw4MCK2fr80YutLeM4555xGr3/00Uc3ennPnj3j2GOPjcceeyw2bNiw298XjogYNWpUDB48OKZOnbrdzuXGVCqVmDlzZpxxxhlNOn5nunbtGpdcckksXbo0nn322RgxYkTMmzcvqqqqoq6uLjp27Fh/7KJFi/bovnZk8+bNsXbt2gYhfu211+rPb0cOOeSQ6NSpU9x11107/PgHRo0aFaNGjYqNGzfGihUr4oYbboixY8dGnz59YtCgQXvpkcCeMwnT5vz1r3+Niy++OMaNGxfLli2L4447LsaMGRNvvvlmRGwN7Kc+9al45pln4oQTTmj0V01NzQ5vf8qUKfHmm2/GxRdf3Ohmp3Xr1sVjjz2203OcOXNmvPTSS3HLLbc06TGdfvrpccYZZ8S0adNi3bp1uzx+06ZNO5wqn3vuuYiI+MQnPhERUf9mKB8s10dsnX5//vOfN+ncPor77ruvwZ/vv//+iNi6pL0jI0eOjL///e/RtWvXRj9njb1pS4cOHWLIkCExc+bMiIh4+umn99pjgL3BJEybsn79+hg9enQcccQRcdttt0V1dXXMnz8/BgwYEBMnTqyf7u64444YPnx4DBs2LCZMmBC9evWK//73v/Hcc8/Fn//851iwYMEO7+Pcc8+NKVOmxPTp0+P555+PCy64II488sjYsGFD/OlPf6rf6fvhlylta/DgwTFq1KjdetOMmTNnxuc+97l444034phjjtnpsW+//Xb06dMnzj333Dj99NPjk5/8ZKxbty6WLl0aN998c9TW1tavApx11llx0003xdixY+Ob3/xmrF27NmbNmhUdOnRo8rntjurq6vjRj34U69atixNPPDH++Mc/xowZM2L48OHxhS98YYfXu+SSS+LBBx+MU089NS699NI47rjj4v33349//etf8dhjj8Vll10WAwcOjGuvvTZefvnlGDp0aPTu3TveeuutuPnmm6OqqiqGDBnSLI8JPrLcfWHQuG13CO/Mh3dHf+1rXysHHnhgWb16dYPjFixYUCKi/PjHP66/7JlnnimjR48u3bt3L1VVVeXQQw8tX/rSl8rtt99ef8yHd0dv63e/+135yle+Unr27FmqqqrKwQcfXAYNGlRuvPHG8s4779Qft+3u6G2tWbOmtGvXbqe7oz9s7NixJSJ2uTt648aNZdasWWX48OHlsMMOKx06dCgdO3YstbW1ZdKkSWXt2rUNjr/rrrvK0UcfXTp06FD69u1bbrjhhjJnzpwSEeXFF1/c5WOJiPLd7363wWUvvvhiiYhy44031l82fvz4ctBBB5VVq1aV0047rXTq1Kl06dKlfPvb3y7r1q1rcP0P744upZR169aVyZMnl6OPPrpUV1eXzp07l2OPPbZceuml9Tvm6+rqyvDhw0uvXr1KdXV16d69exkxYkRZtmzZTp8zyFApZScvHgTYiyZMmBALFy5s0pI67A98TxgAkogwACSxHA0ASUzCAJBEhAEgiQgDQBIRBoAkTX7HrMZ+eDkA0Lim7Hs2CQNAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASBJ++wToG0rpdT/vlKpJJ4JQMtjEgaAJCIMAElEGACSiDAAJBFhAEgiwgCQxEuUaFZelgSwYyZhAEgiwgCQRIQBIIkIA0ASEQaAJCIMAElEGACSiDAAJBFhAEgiwgCQRIQBIIkIA0ASEQaAJCIMAElEGACSiDAAJBFhAEgiwgCQRIQBIIkIA0CS9tknAPurUkr97yuVSuKZAFlMwgCQxCQM+9i2EzCwfzMJA0ASEQaAJJajgVansSV9m9tojUzCAJDEJAz7mIltz3kOaStMwgCQRIQBIIkIA0ASEQaAJCIMAElEGACSiDAAJBFhAEgiwgCQRIQBIIkIA0ASEQaAJCIMTVRKafRH6AF8VCIMAEn8KENoIj8+D9jbTMIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJmjXCpZQopTTnXQBAq2USBoAkIgwASUQYAJKIMAAkEWEASNJ+X9zJrnZIVyqVfXEaANCimIQBIEmzTsL7YsJtbMo2WQPQGpiEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIsk/eMas5eU0wAK2VSRgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBI0j77BGg+pZQmHVepVJr5TABojEkYAJKIMAAksRxNg2VrS9PA/irjW3gmYQBIYhJuw0y1AC2bSRgAkogwACSxHA0AkfMtPJMwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkKR99gnQfEop211WqVQSzgSAxpiEASCJSbgN23bqbWwqBiCXSRgAkogwACSxHL2fsCELoOUxCQNAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkrTPPgEA2NtKKc1225VKZa/dlkkYAJKIMAAkEWEASCLCAJDExiyAVm7bTUh7c9MQ/19zPa8mYQBIIsIAkMRyNEAbsievj7WUvb3mfk5MwgCQxCQM0MptO6015ztFsfeZhAEgiQgDQBLL0QBtyEdZmm6LG7Ka+jxkP3aTMAAkMQkDe1VjU0f2tLG/8rxv1ZKfB5MwACQRYQBIYjkagL2qOV+r3JKXlj8KkzAAJBFhAEgiwgCQRIQBIImNWTSL/fmdemB/599105mEASBJi5+EdzRR+Z8WAK2dSRgAkogwACRpscvRzfmOKwDQEpiEASBJi52Eabtsqmt7rFzBR2MSBoAkIgwASVpshCuVimXLVsznD2DXWmyEAaCtszGLfcZkDNCQSRgAkogwACSxHE2zsgS9f/B5ho/GJAwASUQYAJK0+OVoy1wAtFUmYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACRpn30CtEyllCYfW6lUmvFMANoukzAAJBFhAEgiwgCQRIQBIImNWTTKZiuA5mcSBoAkIgwASUQYAJKIMAAkEWEASCLCAJBEhAEgiQgDQBIRBoAkIgwASUQYAJKIMAAkEWEASCLCAJBEhAEgiQgDQBIRBoAk7bNPAKCtKKXU/75SqSSeCa2FSRgAkogwACQRYQBIIsIAkMTGLIA9sO1mLNhdJmEASCLCAJBEhAEgiQgDQBIbs5pgRxsvvCMOQI7Gvi63xq/JJmEASCLCAJDEcjQArUZbe122SRgAkpiEAfZAa9wMRMthEgaAJCIMAElEGACSiDAAJLExC4B0beUdsHaXSRgAkogwACQRYQBIIsIAkESEASCJ3dEApNsfdkI3xiQMAElEGACSiDAAJBFhAEhiYxYAbUJjb325K9kbwkzCAJBEhAEgiQgDQBIRBoAkIgwASUQYAJKIMAAkEWEASCLCAJDEO2Y1QfY7qgCwVVv7emwSBoAkIgwASUQYAJKIMAAkEWEASCLCAJBEhAEgiQgDQBIRBoAkIgwASUQYAJKIMAAkEWEASCLCAJBEhAEgiQgDQBIRBoAk7bNPAABaglLKbh1fqVT2+D5NwgCQRIQBIIkIA0ASEQaAJCIMAElEGACSeIkSAOyGvfHSpA+YhAEgiQgDQBIRBoAkIgwASUSY/V4pZbffMxZgbxBhAEgiwgCQxOuE4f/Zdkl6b74OEGj9mutrgkkYAJKYhNnvmXqBLCZhAEgiwgCQxHI0e6ypr7G17AvQkEkYAJKIMADE1tW6fb1iJ8IAkESEASCJjVnsMRuuaE578sM1/N1kTzX33yGTMAAkMQkDLYYfKUlLsC9XUEzCAJBEhAEgieVooFVobIlwR8vXNmTRWpiEASCJCANAEsvRQIthGZn9jUkYAJKIMAAkEWEASCLCAJDExiygTbCpi9bIJAwASUQYAJKIMAAkEWEASCLCAJBEhAEgiZcoAa2WlyXR2pmEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkogwACQRYQBIIsIAkESEASBJ+6YeWEppzvMAgP2OSRgAkogwACQRYQBIIsIAkESEASCJCANAEhEGgCQiDABJRBgAkvwfS6l3MFfH3w4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# 1) Hyperparameters\n",
    "# -----------------------\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() \n",
    "                      else 'mps' if torch.backends.mps.is_available() \n",
    "                      else 'cpu')\n",
    "\n",
    "# -----------------------\n",
    "# 2) Data: MNIST\n",
    "# -----------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()   # yields a tensor in [0,1]\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='mnist_data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='mnist_data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Each sample is shape (1, 28, 28).  We keep it in 2D form for PixelCNN.\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3) Masked Convolution: The Core of PixelCNN\n",
    "# ------------------------------------------------------\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    A 2D convolution where the kernel is masked so that \n",
    "    the output pixel cannot depend on 'future' pixels.\n",
    "\n",
    "    mask_type: 'A' or 'B'\n",
    "      - 'A' is used for the first conv layer so it doesn't see the current pixel.\n",
    "      - 'B' is used for subsequent layers; it can see the current pixel but not future ones.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, mask_type, \n",
    "                 stride=1, padding=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                         stride=stride, padding=padding, dilation=dilation, \n",
    "                         groups=groups, bias=bias)\n",
    "        assert mask_type in ['A','B'], \"mask_type must be 'A' or 'B'\"\n",
    "        self.mask_type = mask_type\n",
    "        \n",
    "        # Register buffer for the mask\n",
    "        self.register_buffer('mask', torch.ones_like(self.weight.data))\n",
    "\n",
    "    def _create_mask(self):\n",
    "        # The shape of weight is (out_channels, in_channels, kH, kW).\n",
    "        kH, kW = self.kernel_size\n",
    "        # The center pixel in the kernel\n",
    "        center_h = kH // 2\n",
    "        center_w = kW // 2\n",
    "\n",
    "        # Construct a mask of ones, then zero out \"future\" pixels.\n",
    "        mask = torch.ones_like(self.weight.data)\n",
    "\n",
    "        # For every output channel, for every input channel, for every kernel row/col:\n",
    "        # We want to zero out the pixels that are below or to the right of the \"current\" pixel.\n",
    "        # Actually, we zero out the center pixel itself only if mask_type=='A'.\n",
    "        \n",
    "        # Indices:\n",
    "        #   everything in row > center_h is future\n",
    "        #   everything in row == center_h and col > center_w is future\n",
    "        # If mask_type='A', also block the center pixel (center_h, center_w).\n",
    "        \n",
    "        mask[:, :, center_h, center_w+1:] = 0  # zero out right half of center row\n",
    "        mask[:, :, center_h+1:, :] = 0        # zero out rows below the center\n",
    "\n",
    "        if self.mask_type == 'A':\n",
    "            # also zero out the center pixel\n",
    "            mask[:, :, center_h, center_w] = 0\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Make sure our stored 'mask' is up to date\n",
    "        if self.mask.sum() == self.mask.numel():  \n",
    "            # i.e. if we haven't yet built it\n",
    "            with torch.no_grad():\n",
    "                self.mask.copy_(self._create_mask())\n",
    "\n",
    "        # apply the mask\n",
    "        self.weight.data *= self.mask\n",
    "        return super().forward(x)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4) PixelCNN Model\n",
    "# ------------------------------------------------------\n",
    "class PixelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple PixelCNN:\n",
    "      - First layer is MaskedConv with 'A'\n",
    "      - Then several MaskedConv with 'B'\n",
    "      - We output 1 channel of logits for each pixel (Bernoulli).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_channels=64, kernel_size=3, num_layers=5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # First layer: type 'A' \n",
    "        self.conv1 = MaskedConv2d(\n",
    "            in_channels=1, out_channels=hidden_channels, \n",
    "            kernel_size=kernel_size, mask_type='A', \n",
    "            padding=kernel_size//2\n",
    "        )\n",
    "\n",
    "        # Middle layers: type 'B'\n",
    "        self.mid_convs = nn.ModuleList()\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.mid_convs.append(\n",
    "                MaskedConv2d(hidden_channels, hidden_channels, \n",
    "                             kernel_size=kernel_size, mask_type='B', \n",
    "                             padding=kernel_size//2)\n",
    "            )\n",
    "\n",
    "        # Last layer: also type 'B', but output 1 channel \n",
    "        # (Bernoulli => each pixel has a single logit)\n",
    "        self.conv_out = MaskedConv2d(\n",
    "            hidden_channels, 1, \n",
    "            kernel_size=kernel_size, mask_type='B',\n",
    "            padding=kernel_size//2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is shape (B,1,28,28), in [0,1]\n",
    "        out = F.relu(self.conv1(x))\n",
    "        for c in self.mid_convs:\n",
    "            out = F.relu(c(out))\n",
    "        out = self.conv_out(out)\n",
    "        # shape = (B,1,28,28).  Interpreted as logits for Bernoulli p.\n",
    "        return out\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5) Training\n",
    "# ------------------------------------------------------\n",
    "def train_pixelcnn(model, train_loader, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(DEVICE)  # shape (B,1,28,28)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(images)  # shape (B,1,28,28)\n",
    "            # Bernoulli NLL with logits => use F.binary_cross_entropy_with_logits\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                logits, images, reduction='mean'\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch}/{epochs}, NLL = {avg_loss:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 6) Testing (optional, we can measure NLL on test set)\n",
    "# ------------------------------------------------------\n",
    "def eval_pixelcnn(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            logits = model(images)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                logits, images, reduction='sum'\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 7) Sampling\n",
    "# ------------------------------------------------------\n",
    "def sample_from_pixelcnn(model, n=8):\n",
    "    \"\"\"\n",
    "    Autoregressive sampling.  \n",
    "    We create an empty canvas (B= n^2, 1, 28, 28) \n",
    "    and fill it in one pixel at a time, left-to-right, top-to-bottom.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = torch.zeros(n*n, 1, 28, 28).to(DEVICE)\n",
    "\n",
    "    # We must fill each pixel [row,col] in order.\n",
    "    for row in range(28):\n",
    "        for col in range(28):\n",
    "            with torch.no_grad():\n",
    "                out = model(samples)  # shape (B,1,28,28) => logits\n",
    "                # get the current pixel's logits\n",
    "                logits = out[:, 0, row, col]\n",
    "                probs = torch.sigmoid(logits)\n",
    "                # sample from Bernoulli\n",
    "                pixel_value = torch.bernoulli(probs)\n",
    "                samples[:, 0, row, col] = pixel_value\n",
    "    return samples.detach().cpu()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 8) Main Script\n",
    "# ------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Using device:\", DEVICE)\n",
    "    model = PixelCNN(hidden_channels=64, kernel_size=3, num_layers=5).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    print(\"Training PixelCNN on MNIST...\")\n",
    "    train_pixelcnn(model, train_loader, optimizer, epochs=EPOCHS)\n",
    "\n",
    "    test_nll = eval_pixelcnn(model, test_loader)\n",
    "    print(f\"Test NLL = {test_nll:.4f}\")\n",
    "\n",
    "    # Generate samples\n",
    "    print(\"Sampling from PixelCNN...\")\n",
    "    sampled_imgs = sample_from_pixelcnn(model, n=6)  # 6x6 grid => 36 samples\n",
    "\n",
    "    # Plot them\n",
    "    grid_img = utils.make_grid(sampled_imgs, nrow=6, padding=2)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(grid_img[0].numpy(), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"PixelCNN Samples\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
